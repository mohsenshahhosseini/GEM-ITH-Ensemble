{\rtf1\ansi\ansicpg1252\cocoartf2580
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Arial-BoldMT;\f1\fswiss\fcharset0 Helvetica-Bold;\f2\fswiss\fcharset0 Arial-ItalicMT;
\f3\fswiss\fcharset0 Helvetica;\f4\fswiss\fcharset0 ArialMT;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid1\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid101\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid2}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\ri0\sl276\slmult1\qc\partightenfactor0

\f0\b\fs28 \cf0 Instructions to run GEM-ITH-Ensemble with Fish_code.py
\f1\fs22 \
\pard\pardeftab720\ri0\sl276\slmult1\partightenfactor0
\cf0 \
\pard\pardeftab720\ri0\sl276\slmult1\partightenfactor0

\f0\fs24 \cf0 Tunable parameters\
\pard\pardeftab720\ri0\sl276\slmult1\partightenfactor0

\f1\fs22 \cf0 \
\pard\pardeftab720\ri0\sl276\slmult1\partightenfactor0

\f2\i\b0 \cf0 \ul \ulc0 GENERAL PARAMETERS
\f3\i0 \ulnone \
\pard\pardeftab720\ri0\sl276\slmult1\partightenfactor0

\f4 \cf0 Line 50: Resampling\
Line 56: Kfold splits\
Line 63: Training size proportion\
\
\pard\pardeftab720\ri0\sl276\slmult1\partightenfactor0

\f2\i \cf0 \ul \ulc0 GEM-ITH - BAYESIAN SEARCH
\f3\i0 \ulnone \
\pard\pardeftab720\ri0\sl276\slmult1\partightenfactor0

\f4 \cf0 Line 77: Number of iterations for Bayesian search\
Line 88: LASSO parameter search space\
Line 107: Random Forest parameter search space\
Line 129: XGBoost parameter search space\
Line 157: SVM parameter search pace\
Line 207: Kfold splits\
\
\pard\pardeftab720\ri0\sl276\slmult1\partightenfactor0

\f2\i \cf0 \ul \ulc0 GEM and STACKING - GRID SEARCH\
\pard\pardeftab720\ri0\sl276\slmult1\partightenfactor0

\f4\i0 \cf0 \ulnone Line 319: LASSO grid search space\
Line 332: Random Forest grid search space\
Line 344: XGBoost grid search space\
Line 357: SVM grid search space\
\
\pard\pardeftab720\ri0\sl276\slmult1\partightenfactor0

\f2\i \cf0 \ul \ulc0 BASE LEARNERS - GRID SEARCH\
\pard\pardeftab720\ri0\sl276\slmult1\partightenfactor0

\f4\i0 \cf0 \ulnone Line 473: LASSO grid search space\
Line 482: Random Forest grid search space\
Line 491: XGBoost grid search space\
Line 501: SVM grid search space\
\pard\pardeftab720\ri0\sl276\slmult1\qj\partightenfactor0

\f3 \cf0 \
\pard\pardeftab720\ri0\sl276\slmult1\partightenfactor0

\f4 \cf0 The code is comprised of 5 main sections as follows:\
\pard\pardeftab720\li720\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls1\ilvl0\cf0 1.	Importing libraries, loading the data, and preprocessing \
2.	GEM-ITH model with hyperparameters tuned by Bayseian search\
3.	GEM model with hyperparameters tuned by Grid search\
4.	Stacked generalization models with hyperparameters tuned by Grid search\
5.	Individual base learners with hyperparameters tuned by Grid search\
\pard\pardeftab720\ri0\sl276\slmult1\qj\partightenfactor0

\f3 \cf0 \
\pard\pardeftab720\ri0\sl276\slmult1\qj\partightenfactor0

\f4 \cf0 This code is meant to be run as one entire piece after the user selects their parameters. However, each section is written in a way that after loading the first section, any of the other sections could be run independently if the user desired. The models\'92 search spaces are defined separately in all sections for two reasons: timing the computation time of all algorithms, and being able to run any of them separately.\
\
After importing necessary modules, lines 36 - 54 load the dataset and create empty dictionaries for storage. Lines 58 - 520 contain the GEM-ITH-ENSEMBLE Algorithm. Lines 67 - 168 tune the models\'92 hyperparameters with Bayesian search. Lines 170-205 define the optimization model. Lines 209 - 268 initiate the GEM-ITH algorithm to find the optimal weights and hyperparameters. Lines 270 - 312 make the final base model predictions on the test set using the found hyperparameters. They then also aggregate the base models predictions with the found optimal weights. Lines 317 - 427 perform a grid search to tune the base models\'92 hyperparameters and then run the GEM algorithm. Lines 429 - 467 create the stacked ensemble models. Lines 469 - 507 perform a grid search for the base learners. Lastly, lines 509 - 552 aggregate the results and save the predictions into a csv file.\
\
\pard\pardeftab720\ri0\sl276\slmult1\partightenfactor0

\f0\b\fs24 \cf0 Instructions on how to run the code\
\
\pard\pardeftab720\li720\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl0
\f4\b0\fs22 \cf0 1.	Load necessary modules\
2.	Place the data set in the same directory as the main Python file (fish_code.py)\
3.	Set the tunable parameters as mentioned in the beginning of this document.\
4.	Run the entire code to generate the results and output csv files.\
}